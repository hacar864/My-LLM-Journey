{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c520dbe",
   "metadata": {},
   "source": [
    "## Exercise 1: Write Translation Functions\n",
    "- Create two translation functions:  \n",
    "  1. **`bert2gpt4()`**: Converts BERT tokens into GPT-4 tokens.  \n",
    "  2. **`gpt42bert()`**: Converts GPT-4 tokens into BERT tokens.  \n",
    "- These functions will serve as bridges between the two tokenization systems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32015828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libs\n",
    "\n",
    "import tiktoken # for GBT4 tokenizer\n",
    "from transformers import BertTokenizer # for BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91fbb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt4_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2191c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "testText = \"Roses are red, herbs are green, why Acar's eyes are not purple?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3886699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 88816, 60, 61741, 527, 2579, 11, 42393, 527, 6307, 11, 3249, 1645, 277, 596, 6548, 527, 539, 25977, 30, 510, 82476, 60]\n",
      "[CLS] roses are red, herbs are green, why acar's eyes are not purple? [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Defining BERT to GBT4 translation function\n",
    "\n",
    "def bert2gbt4(bertToks):\n",
    "    text = bert_tokenizer.decode(bertToks)\n",
    "    c = gbt4_tokenizer.encode(text)\n",
    "    return c\n",
    "\n",
    "BERTtoks=bert_tokenizer.encode(testText)\n",
    "gbtToks=bert2gbt4(BERTtoks)\n",
    "print(gbtToks)\n",
    "print(gbt4_tokenizer.decode(gbtToks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "901df5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10529, 2024, 2417, 1010, 17561, 2024, 2665, 1010, 2339, 9353, 2906, 1005, 1055, 2159, 2024, 2025, 6379, 1029]\n",
      "roses are red, herbs are green, why acar's eyes are not purple?\n"
     ]
    }
   ],
   "source": [
    "# Defining GBT4 to BERT translation function\n",
    "\n",
    "def gbt42bert(gbt4Toks):\n",
    "    text = gbt4_tokenizer.decode(gbt4Toks)\n",
    "    b = bert_tokenizer.encode(text)\n",
    "    return b[1:-1]\n",
    "\n",
    "GBT4tokens = gbt4_tokenizer.encode(testText)\n",
    "bertToks = gbt42bert(GBT4tokens)\n",
    "print(bertToks)\n",
    "print(bert_tokenizer.decode(bertToks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b511b",
   "metadata": {},
   "source": [
    "## Exercise 2: BERT ➡ GPT-4 ➡ BERT \n",
    "- Convert BERT tokens into GPT-4 tokens using the `bert2gpt4()` function.  \n",
    "- Then, translate back to BERT tokens using `gpt42bert()`.  \n",
    "- Validate the round-trip conversion to ensure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7fa2da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "\tWorld is so violent, I shouldn't be so humble to anyone.\n",
      "\n",
      "BERT tokens:\n",
      "\t[2088, 2003, 2061, 6355, 1010, 1045, 5807, 1005, 1056, 2022, 2061, 15716, 2000, 3087, 1012]\n",
      "\n",
      "BERT to GBT4:\n",
      "\t[CLS] roses are red, herbs are green, why acar's eyes are not purple? [SEP]\n",
      "\n",
      "Back to BERT:\n",
      "\tworld is so violent, i shouldn't be so humble to anyone.\n"
     ]
    }
   ],
   "source": [
    "text01 = \"World is so violent, I shouldn't be so humble to anyone.\"\n",
    "\n",
    "print(f\"Original text:\\n\\t{text01}\")\n",
    "\n",
    "bertToks01 = bert_tokenizer.encode(text01)[1:-1]\n",
    "print(f\"\\nBERT tokens:\\n\\t{bertToks01}\")\n",
    "\n",
    "gbtToks01 = bert2gbt4(bertToks01)\n",
    "print(f\"\\nBERT to GBT4:\\n\\t{ gbt4_tokenizer.decode(gbtToks) }\")\n",
    "\n",
    "back_to_bertToks = gbt42bert(gbtToks01)\n",
    "print(f\"\\nBack to BERT:\\n\\t{ bert_tokenizer.decode(back_to_bertToks) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd731da2",
   "metadata": {},
   "source": [
    "## Exercise 3: GPT-4 ➡ BERT ➡ GPT-4\n",
    "- Start with GPT-4 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3263c04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "\tCan a human live 300 years?\n",
      "\n",
      "GBT4 tokens:\n",
      "\t[6854, 264, 3823, 3974, 220, 3101, 1667, 30]\n",
      "\n",
      "GBT4 to BERT:\n",
      "\tcan a human live 300 years?\n",
      "\n",
      "Back to GBT4:\n",
      "\tcan a human live 300 years?\n"
     ]
    }
   ],
   "source": [
    "text02 = \"Can a human live 300 years?\"\n",
    "\n",
    "print(f\"Original text:\\n\\t{text02}\")\n",
    "\n",
    "gbtToks02 = gbt4_tokenizer.encode(text02)\n",
    "print(f\"\\nGBT4 tokens:\\n\\t{gbtToks02}\")\n",
    "\n",
    "bertToks02 = gbt42bert(gbtToks02)\n",
    "print(f\"\\nGBT4 to BERT:\\n\\t{bert_tokenizer.decode(bertToks02)}\")\n",
    "\n",
    "back_to_gbtToks = bert2gbt4(bertToks02)\n",
    "print(f\"\\nBack to GBT4:\\n\\t{gbt4_tokenizer.decode(back_to_gbtToks)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
