{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f65a89",
   "metadata": {},
   "source": [
    "## Exercise 1: Analyze Book Compression\n",
    "- **Import the GPT-4 tokenizer.**\n",
    "- Import several books using their URLs.\n",
    "- Calculate **compression ratio** as `tokens / characters`.\n",
    "- Display the results in a **table format** for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09259424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libs\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import tiktoken\n",
    "from urllib.parse import urlparse\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3cb969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100277"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt4_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "gbt4_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a52a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all books have the same url format;\n",
    "# they are unique by numerical code\n",
    "baseurl = 'https://www.gutenberg.org/cache/epub/'\n",
    "\n",
    "bookurls = [\n",
    "    # code       title\n",
    "    ['84',    'Frankenstein'    ],\n",
    "    ['64317', 'GreatGatsby'     ],\n",
    "    ['11',    'AliceWonderland' ],\n",
    "    ['1513',  'RomeoJuliet'     ],\n",
    "    ['76',    'HuckFinn'        ],\n",
    "    ['219',   'HeartDarkness'   ],\n",
    "    ['2591',  'GrimmsTales'     ],\n",
    "    ['2148',  'EdgarAllenPoe'   ],\n",
    "    ['36',    'WarOfTheWorlds'  ],\n",
    "    ['829',   'GulliversTravels']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9dabab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "infos = np.zeros( (len(bookurls),3) )\n",
    "\n",
    "for idx,(c,b) in enumerate(bookurls):\n",
    "    \n",
    "    text = ( requests.get(baseurl+c+\"/pg\"+c+\".txt\") ).text\n",
    "    tokens = gbt4_tokenizer.encode(text)\n",
    "\n",
    "    compression = len(tokens)/len(text)\n",
    "\n",
    "    infos[idx,1] = len(tokens)\n",
    "    infos[idx,0] = len(text)\n",
    "    infos[idx,2] = compression*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c871827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Book Title       | Characters  | Tokens    | Compression |\n",
      "------------------------------------------------------------\n",
      "| Frankenstein     | 446,544     | 102,419   |      22.94% |\n",
      "\n",
      "| GreatGatsby      | 296,858     | 70,343    |      23.70% |\n",
      "\n",
      "| AliceWonderland  | 167,674     | 41,457    |      24.72% |\n",
      "\n",
      "| RomeoJuliet      | 167,426     | 43,761    |      26.14% |\n",
      "\n",
      "| HuckFinn         | 602,714     | 159,125   |      26.40% |\n",
      "\n",
      "| HeartDarkness    | 232,885     | 56,483    |      24.25% |\n",
      "\n",
      "| GrimmsTales      | 549,736     | 137,252   |      24.97% |\n",
      "\n",
      "| EdgarAllenPoe    | 632,131     | 144,315   |      22.83% |\n",
      "\n",
      "| WarOfTheWorlds   | 363,420     | 84,580    |      23.27% |\n",
      "\n",
      "| GulliversTravels | 611,742     | 143,560   |      23.47% |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"| Book Title       | Characters  | Tokens    | Compression |\\n\"+\"-\"*60)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"| {bookurls[i][1]:16} | {int(infos[i,0]):<11,} | {int(infos[i,1]):<8,}  | {infos[i,2]:>10.2f}% |\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62724bed",
   "metadata": {},
   "source": [
    "## Exercise 2: Analyze Website Compression\n",
    "- Use websites as input text data.\n",
    "- **Tip:** Use `urllib.parse` to clean and process the URLs.\n",
    "- Compute and display compression ratios for different websites in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5958cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weburls = [\n",
    "    'http://python.org/',\n",
    "    'https://pytorch.org/',\n",
    "    'https://en.wikipedia.org/wiki/List_of_English_words_containing_Q_not_followed_by_U',\n",
    "    'https://sudoku.com/',\n",
    "    'https://reddit.com/',\n",
    "    'https://visiteurope.com/en/',\n",
    "    'https://sincxpress.com/',\n",
    "    'https://openai.com/',\n",
    "    'https://theuselessweb.com/',\n",
    "    'https://maps.google.com/',\n",
    "    'https://pigeonsarentreal.co.uk/',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09931673",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_infos = np.zeros( (len(weburls),3) )\n",
    "\n",
    "for idx,web in enumerate(weburls):\n",
    "    \n",
    "    text = requests.get(web).text\n",
    "    tokens = gbt4_tokenizer.encode(text)\n",
    "\n",
    "    compression = len(tokens)/len(text)\n",
    "\n",
    "    web_infos[idx,1] = len(tokens)\n",
    "    web_infos[idx,0] = len(text)\n",
    "    web_infos[idx,2] = compression*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2583bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Websites               | Characters  | Tokens    | Compression |\n",
      "-------------------------------------------------------------------\n",
      "| python.org             | 50,172      | 12,781    |      25.47% |\n",
      "\n",
      "| pytorch.org            | 388,671     | 111,425   |      28.67% |\n",
      "\n",
      "| en.wikipedia.org       | 92          | 26        |      28.26% |\n",
      "\n",
      "| sudoku.com             | 145,997     | 52,589    |      36.02% |\n",
      "\n",
      "| reddit.com             | 460,477     | 143,867   |      31.24% |\n",
      "\n",
      "| visiteurope.com        | 124,950     | 34,344    |      27.49% |\n",
      "\n",
      "| sincxpress.com         | 25,580      | 6,843     |      26.75% |\n",
      "\n",
      "| openai.com             | 11,533      | 6,398     |      55.48% |\n",
      "\n",
      "| theuselessweb.com      | 4,756       | 1,329     |      27.94% |\n",
      "\n",
      "| maps.google.com        | 211,258     | 107,369   |      50.82% |\n",
      "\n",
      "| pigeonsarentreal.co.uk | 243,863     | 71,232    |      29.21% |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"| Websites               | Characters  | Tokens    | Compression |\\n\"+\"-\"*67)\n",
    "\n",
    "for i in range(11):\n",
    "    print(f\"| {urlparse(weburls[i]).hostname:22} | {int(web_infos[i,0]):<11,} | {int(web_infos[i,1]):<8,}  | {web_infos[i,2]:>10.2f}% |\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ebc14f",
   "metadata": {},
   "source": [
    "## Exercise 3: Analyze String Library Attributes\n",
    "- Use all attributes from the Python `string` library as the dataset.\n",
    "- Calculate and display their compression ratios.\n",
    "- **Tip:** Use `dir(string)` to access all string attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ccb47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Attribute       | Characters | Tokens | Compression |\n",
      "-------------------------------------------------------\n",
      "| __name__        |          6 |      1 |  16.67%     |\n",
      "| __doc__         |        622 |    109 |  17.52%     |\n",
      "| __file__        |         38 |     12 |  31.58%     |\n",
      "| __cached__      |         63 |     22 |  34.92%     |\n",
      "| whitespace      |          6 |      4 |  66.67%     |\n",
      "| ascii_lowercase |         26 |      1 |   3.85%     |\n",
      "| ascii_uppercase |         26 |      1 |   3.85%     |\n",
      "| ascii_letters   |         52 |      2 |   3.85%     |\n",
      "| digits          |         10 |      4 |  40.00%     |\n",
      "| hexdigits       |         22 |      7 |  31.82%     |\n",
      "| octdigits       |          8 |      3 |  37.50%     |\n",
      "| punctuation     |         32 |     21 |  65.62%     |\n",
      "| printable       |        100 |     31 |  31.00%     |\n"
     ]
    }
   ],
   "source": [
    "# I got some help in this exercise (I should dig OOP)\n",
    "\n",
    "print(\"| Attribute       | Characters | Tokens | Compression |\\n\"+\"-\"*55)\n",
    "\n",
    "for k,v in string.__dict__.items():\n",
    "  if isinstance(v,str) and (len(v)>0):\n",
    "\n",
    "    # get the text\n",
    "    num_chars = len(v)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = gbt4_tokenizer.encode(v)\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    # compression ratio\n",
    "    compress = 100*num_tokens/num_chars\n",
    "\n",
    "    print(f'| {k:15} | {num_chars:>10,} | {num_tokens:>6,} |  {compress:>5.2f}%     |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
